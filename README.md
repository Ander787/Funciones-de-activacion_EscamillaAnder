# README - Funciones de ActivaciÃ³n en Redes Neuronales

## ğŸ¤“ InformaciÃ³n del Proyecto  
**Materia:** Sistemas de VisiÃ³n Artificial  
**Tarea:** Tarea 2.1 - GrÃ¡ficas de funciones de activaciÃ³n con su derivada  
**Estudiante:** Ander Heinrich Escamilla Wong
**Fecha:** 03/03/2025  

---

## âœï¸ DescripciÃ³n General  
Este repositorio contiene un conjunto de scripts en Python que grafican diversas funciones de activaciÃ³n junto con sus derivadas. Cada funciÃ³n estÃ¡ implementada en su propio mÃ³dulo dentro de la carpeta `src/`.  

El archivo `main.py` es el punto de entrada para ejecutar todas las grÃ¡ficas de manera conjunta.  

---

## ğŸ§  Funciones de ActivaciÃ³n en Redes Neuronales  
Este proyecto implementa las siguientes funciones de activaciÃ³n:  

- **Sigmoide**  
- **ReLU (Rectified Linear Unit)**  
- **Leaky ReLU**  
- **TanH (Tangente HiperbÃ³lica)**  
- **ELU (Exponential Linear Unit)**  
- **Softmax**  

Cada funciÃ³n tiene su respectiva derivada y se representa visualmente en los grÃ¡ficos generados por el cÃ³digo.  

---

# Proyecto de Funciones MatemÃ¡ticas

Este repositorio contiene la implementaciÃ³n de diversas funciones matemÃ¡ticas utilizadas en modelos de aprendizaje automÃ¡tico y redes neuronales.

## â˜ï¸ Requisitos Previos
Antes de ejecutar el cÃ³digo, asegÃºrate de tener instaladas las siguientes bibliotecas en tu entorno de Python:

```sh
pip install numpy matplotlib
```

## ğŸ“‹ Estructura del Repositorio

```
proyecto_funciones/
â”‚â”€â”€ src/                        # CÃ³digo fuente
â”‚   â”œâ”€â”€ Escalon.py              # FunciÃ³n EscalÃ³n
â”‚   â”œâ”€â”€ Gaussiana.py            # FunciÃ³n Gaussiana
â”‚   â”œâ”€â”€ Identidad.py            # FunciÃ³n Identidad
â”‚   â”œâ”€â”€ Lineal_a_tramos.py      # FunciÃ³n Lineal a Tramos
â”‚   â”œâ”€â”€ Relu.py                 # FunciÃ³n ReLU
â”‚   â”œâ”€â”€ Sigmoidal.py            # FunciÃ³n Sigmoide
â”‚   â”œâ”€â”€ Sinusoidal.py           # FunciÃ³n Sinusoidal
â”‚   â”œâ”€â”€ Tangente_hiperbolica.py # FunciÃ³n Tangente HiperbÃ³lica
â”‚â”€â”€ main.py                     # Punto de entrada para ejecutar todas las funciones
â”‚â”€â”€ README.md                   # Este documento
```

### 1. `main.py`
Este archivo es el punto de entrada del proyecto. Su funciÃ³n principal es importar y ejecutar todas las funciones implementadas en `src/`. Para ello, realiza las siguientes importaciones:

```python
from src.Gaussiana import plot_gaussian
from src.Escalon import plot_step_function
from src.Identidad import plot_identidad
from src.Lineal_a_tramos import plot_piecewise_func
from src.Relu import plot_relu
from src.Sigmoidal import plot_sigmoid
from src.Sinusoidal import plot_sinusoidal
from src.Tangente_hiperbolica import plot_tanh
```

### 2. Funciones Implementadas
Cada funciÃ³n implementada genera una grÃ¡fica de la funciÃ³n de activaciÃ³n correspondiente, junto con su derivada. 

#### a) FunciÃ³n EscalÃ³n (`escalon.py`)
Representa la funciÃ³n de Heaviside, una funciÃ³n discontinua usada en redes neuronales.

```python
def step_function(x):
    return np.where(x < 0, 0, 1)

def step_derivative(x):
    return np.zeros_like(x)
```

#### b) FunciÃ³n Gaussiana (`gaussiana.py`)

```python
def gaussian_function(x):
    return np.exp(-x**2)

def gaussian_derivative(x):
    return -2 * x * np.exp(-x**2)
```

#### c) FunciÃ³n Identidad (`identidad.py`)

```python
def identidad_function(x):
    return x

def identidad_derivative(x):
    return np.ones_like(x)
```

#### d) FunciÃ³n Lineal a Tramos (`lineal_a_tramos.py`)

```python
def piecewise_function(x):
    return np.piecewise(x, [x < 0, x >= 0], [lambda x: x, lambda x: 2 * x])

def piecewise_derivative(x):
    return np.piecewise(x, [x < 0, x >= 0], [1, 2])
```

#### e) FunciÃ³n ReLU (`relu.py`)

```python
def relu_function(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)
```

#### f) FunciÃ³n Sigmoide (`sigmoidal.py`)

```python
def sigmoid_function(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid_function(x)
    return s * (1 - s)
```

#### g) FunciÃ³n Sinusoidal (`sinusoidal.py`)

```python
def sinusoidal_function(x):
    return np.sin(x)

def sinusoidal_derivative(x):
    return np.cos(x)
```

#### h) FunciÃ³n Tangente HiperbÃ³lica (`tangente_hiperbolica.py`)

```python
def tanh_function(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2
```

## ğŸ“– Uso
Para ejecutar todas las funciones de activaciÃ³n, usa el siguiente comando:

```sh
python main.py
```

Esto generarÃ¡ grÃ¡ficos de todas las funciones definidas en `src/`.

## ğŸ“Š Resultados Esperados

El programa generarÃ¡ **8 grÃ¡ficas** que muestran:
- Funciones de activaciÃ³n comunes.
- Sus derivadas correspondientes, calculadas numÃ©ricamente.

## ğŸ” ConclusiÃ³n

El proyecto visualiza funciones de activaciÃ³n y sus derivadas con NumPy y Matplotlib, ofreciendo una herramienta clara para analizar su rol en redes neuronales y transformaciones matemÃ¡ticas.



## â­ Â¡Dale una estrella al repo si te fue Ãºtil!
